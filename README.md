# ğŸ›¡ï¸ Deploying a Hugging Face PEFT Model with Accelerate on AWS SageMaker

## ğŸš€ Overview

This project demonstrates how to fine-tune a BERT model using Parameter-Efficient Fine-Tuning (PEFT) techniques with Hugging Face's `transformers`, `accelerate`, and `peft` libraries, and deploy it as a scalable web service on AWS SageMaker.

The model is trained to perform **Toxic Comment Classification**, detecting harmful or offensive content in user-generated text.

---

## ğŸ¯ Goals

- Fine-tune a BERT model for toxic comment classification  
- Use PEFT (LoRA) for parameter-efficient training  
- Train using Hugging Face Accelerate  
- Serve the model via a FastAPI app  
- Containerize the app using Docker  
- Deploy to AWS SageMaker (or EC2)  
- Monitor with AWS CloudWatch  
- Document workflows in GitHub + Colab + blog  

---

## ğŸ§  Model

- **Base Model:** [`bert-base-uncased`](https://huggingface.co/bert-base-uncased)  
- **Task:** Binary text classification (toxic / non-toxic)  
- **Fine-tuning Technique:** LoRA (Low-Rank Adaptation) via Hugging Face `peft`  
- **Frameworks:** `transformers`, `datasets`, `accelerate`, `peft`, `FastAPI`  

---

## ğŸ“Š Dataset

- **Source:** [Civil Comments](https://huggingface.co/datasets/civil_comments) via Hugging Face Datasets  
- **Size:** ~400K user comments  
- **Labels:** Binary (toxic or not)  
- **Preprocessing:** Tokenization with `BertTokenizer`, truncation/padding, label encoding  

---

## ğŸ—ï¸ Project Structure

```
toxic-comment-bert-peft/
â”‚
â”œâ”€â”€ train.py                    # Fine-tuning script using Accelerate + PEFT
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ app.py                  # FastAPI server exposing /predict
â”‚   â””â”€â”€ model.py                # Loads tokenizer and model, handles predictions
â”œâ”€â”€ Dockerfile                  # Container config for serving the FastAPI app
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ sagemaker/
â”‚   â”œâ”€â”€ deploy.py               # Script to deploy to AWS SageMaker
â”‚   â””â”€â”€ config/                 # IAM roles, endpoint configs, etc.
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ cloudwatch_setup.py     # Set up logging and alarms with AWS CloudWatch
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ inference_demo.ipynb    # Colab notebook for live inference demo
â”œâ”€â”€ README.md                   # Project documentation
â””â”€â”€ .gitignore
```

---

## ğŸ§ª Training

```bash
accelerate launch train.py
```

This command runs distributed training using Hugging Face Accelerate. PEFT with LoRA reduces memory consumption and training time while preserving model performance.

---

## ğŸŒ Serving the Model (Locally)

```bash
uvicorn inference.app:app --host 0.0.0.0 --port 8000 --reload
```

Once launched, send a POST request to `/predict` with JSON input:
```json
{
  "text": "You are a terrible person!"
}
```

Response:
```json
{
  "toxic": true,
  "confidence": 0.92
}
```

---

## ğŸ³ Docker Container

To build and run the model server in a container:

```bash
docker build -t toxic-api .
docker run -p 8000:8000 toxic-api
```

---

## â˜ï¸ AWS SageMaker Deployment (Optional)

This project supports cloud deployment to **Amazon SageMaker** via the `sagemaker/` scripts. Steps include:

1. Upload the model to S3  
2. Create a SageMaker inference endpoint  
3. Deploy the Docker image  
4. Monitor usage with CloudWatch  

---

## ğŸ“ˆ Monitoring with AWS CloudWatch

- Automatically tracks:
  - Request latency
  - Error rate
  - Invocations per second
- Custom logs include input samples and predictions for traceability

---

## ğŸ““ Colab / Hugging Face Spaces

A Colab notebook is provided to:

- Load the fine-tuned model from Hugging Face Hub or S3  
- Run live predictions on user text  
- Compare toxic vs non-toxic outputs  
- Visualize confidence scores and embeddings  

> Optionally, a Hugging Face Space demo can be created using Gradio for a web UI.

---

## ğŸ§  Extensions / Future Work

- Multi-label toxicity (threat, insult, obscene, etc.)
- Support multilingual classification with `bert-base-multilingual-cased`
- Integrate a frontend with Streamlit or Gradio
- Add unit testing and CI/CD (GitHub Actions)

---

## âœï¸ Author

**Samuel Diop**  
- ğŸ”— [LinkedIn](https://www.linkedin.com/in/samuel-diop/)  
- ğŸ’» [Portfolio](http://samueldiop.com)  
- ğŸ§  [GitHub](https://github.com/Slownite)

---

## ğŸ“„ License

MIT License. Feel free to reuse and contribute.
